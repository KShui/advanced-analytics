{"cells":[{"cell_type":"markdown","metadata":{"id":"X_aR9xT5NrKC"},"source":["# 3. Spark\n","\n","Spark Programming Guide: <https://spark.apache.org/docs/latest/> (use Python API recommended)\n","Spark API: <https://spark.apache.org/docs/latest/api/python/index.html>\n","\n","\n","# 3.1 Example Walkthrough\n","3.1 Follow the Spark Examples below! After completion see Exercise 3.2 and 3.3!\n","\n","\n","### Initialize PySpark\n","\n","First, we use the findspark package to initialize PySpark."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40046,"status":"ok","timestamp":1615716675783,"user":{"displayName":"Andre Luckow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64","userId":"13897213013486729084"},"user_tz":-60},"id":"g5Zw7iFCN0ED","outputId":"0555b7aa-9b20-4908-9fc3-a1423d8bc189"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pyspark in /Users/q224516/anaconda3/envs/aaml2023/lib/python3.10/site-packages (3.3.2)\n","Requirement already satisfied: py4j==0.10.9.5 in /Users/q224516/anaconda3/envs/aaml2023/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"pirTc4KfNrKG"},"outputs":[],"source":["import os, sys"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7473,"status":"ok","timestamp":1615716742055,"user":{"displayName":"Andre Luckow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64","userId":"13897213013486729084"},"user_tz":-60},"id":"PQrr41GwNrKI","outputId":"a187f48e-d995-4db0-d397-7d5305823f4e"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]},{"name":"stdout","output_type":"stream","text":["23/03/04 18:26:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","23/03/04 18:26:21 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n","PySpark initiated...\n"]}],"source":["# Initialize PySpark\n","APP_NAME = \"PySpark Lecture\"\n","SPARK_MASTER=\"local[1]\"\n","import pyspark\n","import pyspark.sql\n","from pyspark.sql import Row\n","conf=pyspark.SparkConf()\n","conf=pyspark.SparkConf().setAppName(APP_NAME).set(\"spark.local.dir\", os.path.join(os.getcwd(), \"tmp\"))\n","sc = pyspark.SparkContext(master=SPARK_MASTER, conf=conf)\n","spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n","\n","print(\"PySpark initiated...\")"]},{"cell_type":"markdown","metadata":{"id":"fGsih3q0NrKI"},"source":["### Hello, World!\n","\n","Loading data, mapping it and collecting the records into RAM..."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1485,"status":"ok","timestamp":1615718551576,"user":{"displayName":"Andre Luckow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64","userId":"13897213013486729084"},"user_tz":-60},"id":"XSNJTxUOU7xG","outputId":"2b279c5a-bac3-4043-fede-99ed74fcda3c"},"outputs":[{"name":"stdout","output_type":"stream","text":["dyld[30313]: Library not loaded: /opt/homebrew/opt/libunistring/lib/libunistring.2.dylib\n","  Referenced from: <A2808CD2-467C-32EF-B7CF-9441D98ABF5E> /opt/homebrew/Cellar/wget/1.21.3/bin/wget\n","  Reason: tried: '/opt/homebrew/opt/libunistring/lib/libunistring.2.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libunistring/lib/libunistring.2.dylib' (no such file), '/opt/homebrew/opt/libunistring/lib/libunistring.2.dylib' (no such file), '/usr/local/lib/libunistring.2.dylib' (no such file), '/usr/lib/libunistring.2.dylib' (no such file, not in dyld cache), '/opt/homebrew/Cellar/libunistring/1.1/lib/libunistring.2.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/Cellar/libunistring/1.1/lib/libunistring.2.dylib' (no such file), '/opt/homebrew/Cellar/libunistring/1.1/lib/libunistring.2.dylib' (no such file), '/usr/local/lib/libunistring.2.dylib' (no such file), '/usr/lib/libunistring.2.dylib' (no such file, not in dyld cache)\n"]}],"source":["!wget https://raw.githubusercontent.com/scalable-infrastructure/exercise-students-2023/master/data/example.csv"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3280,"status":"ok","timestamp":1615716794158,"user":{"displayName":"Andre Luckow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64","userId":"13897213013486729084"},"user_tz":-60},"id":"Pt0wm3GQNrKI","outputId":"83d24461-1f0b-44db-bd61-c0a3e116cd04"},"outputs":[{"ename":"Py4JJavaError","evalue":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/q224516/Library/CloudStorage/GoogleDrive-andre.luckow@gmail.com/My Drive/02_LMU/VL/2023SS_VL_AAML/code/exercise-students-2023/03_Spark/example.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: Input path does not exist: file:/Users/q224516/Library/CloudStorage/GoogleDrive-andre.luckow@gmail.com/My Drive/02_LMU/VL/2023SS_VL_AAML/code/exercise-students-2023/03_Spark/example.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 32 more\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m data \u001b[39m=\u001b[39m csv_lines\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m line: line\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[39m# Collect the dataset into local RAM\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m data\u001b[39m.\u001b[39;49mcollect()\n","File \u001b[0;32m~/anaconda3/envs/aaml2023/lib/python3.10/site-packages/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[1;32m   1198\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n","File \u001b[0;32m~/anaconda3/envs/aaml2023/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n","File \u001b[0;32m~/anaconda3/envs/aaml2023/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n","File \u001b[0;32m~/anaconda3/envs/aaml2023/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/q224516/Library/CloudStorage/GoogleDrive-andre.luckow@gmail.com/My Drive/02_LMU/VL/2023SS_VL_AAML/code/exercise-students-2023/03_Spark/example.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: Input path does not exist: file:/Users/q224516/Library/CloudStorage/GoogleDrive-andre.luckow@gmail.com/My Drive/02_LMU/VL/2023SS_VL_AAML/code/exercise-students-2023/03_Spark/example.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 32 more\n"]}],"source":["# Load the text file using the SparkContext\n","csv_lines = sc.textFile(\"example.csv\")\n","\n","# Map the data to split the lines into a list\n","data = csv_lines.map(lambda line: line.split(\",\"))\n","\n","# Collect the dataset into local RAM\n","data.collect()"]},{"cell_type":"markdown","metadata":{"id":"c6IFRE0kNrKJ"},"source":["### Creating Objects from CSV\n","\n","Using a function with a map operation to create objects (dicts) as records..."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1203,"status":"ok","timestamp":1615716800399,"user":{"displayName":"Andre Luckow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64","userId":"13897213013486729084"},"user_tz":-60},"id":"wFzFYn0MNrKJ","outputId":"d3e023c8-6807-4013-a633-3f52ee418675"},"outputs":[{"data":{"text/plain":["{'company': 'Relato', 'name': 'Russell Jurney', 'title': 'CEO'}"]},"execution_count":5,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# Turn the CSV lines into objects\n","def csv_to_record(line):\n","    parts = line.split(\",\")\n","    record = {\n","      \"name\": parts[0],\n","      \"company\": parts[1],\n","      \"title\": parts[2]\n","    }\n","    return record\n","\n","# Apply the function to every record\n","records = csv_lines.map(csv_to_record)\n","\n","# Inspect the first item in the dataset\n","records.first()"]},{"cell_type":"markdown","metadata":{"id":"ZjMBodwrNrKJ"},"source":["### GroupBy\n","\n","Using the groupBy operator to count the number of jobs per person..."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1947,"status":"ok","timestamp":1615716805677,"user":{"displayName":"Andre Luckow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64","userId":"13897213013486729084"},"user_tz":-60},"id":"NfOqDcz6NrKJ","outputId":"51336ca5-94a9-4b19-8a03-7a3aa8a30369"},"outputs":[{"data":{"text/plain":["[{'job_count': 2, 'name': 'Russell Jurney'},\n"," {'job_count': 1, 'name': 'Florian Liebert'},\n"," {'job_count': 1, 'name': 'Don Brown'},\n"," {'job_count': 1, 'name': 'Steve Jobs'},\n"," {'job_count': 1, 'name': 'Donald Trump'}]"]},"execution_count":6,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# Group the records by the name of the person\n","grouped_records = records.groupBy(lambda x: x[\"name\"])\n","\n","# Show the first group\n","grouped_records.first()\n","\n","# Count the groups\n","job_counts = grouped_records.map(\n","  lambda x: {\n","    \"name\": x[0],\n","    \"job_count\": len(x[1])\n","  }\n",")\n","\n","job_counts.first()\n","\n","job_counts.collect()"]},{"cell_type":"markdown","metadata":{"id":"unQf6WJhNrKK"},"source":["### Map vs FlatMap\n","\n","Understanding the difference between the map and flatmap operators..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0W5jyqtNrKK"},"outputs":[],"source":["# Compute a relation of words by line\n","words_by_line = csv_lines\\\n","  .map(lambda line: line.split(\",\"))\n","\n","print(words_by_line.collect())\n","\n","# Compute a relation of words\n","flattened_words = csv_lines\\\n","  .map(lambda line: line.split(\",\"))\\\n","  .flatMap(lambda x: x)\n","\n","flattened_words.collect()"]},{"cell_type":"markdown","metadata":{"id":"SnQTDbiWNrKK"},"source":["---\n","## Further Exercises\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51092,"status":"ok","timestamp":1615718101014,"user":{"displayName":"Andre Luckow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64","userId":"13897213013486729084"},"user_tz":-60},"id":"HrTk8P9iORnI","outputId":"53e1cbc2-ad79-4bb9-c0e4-c7c8393b8e2c"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2021-03-14 10:34:10--  https://raw.githubusercontent.com/scalable-infrastructure/exercise-students-2021/master/data/nasa/NASA_access_log_Jul95.gz\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 20676677 (20M) [application/octet-stream]\n","Saving to: ‘NASA_access_log_Jul95.gz’\n","\n","NASA_access_log_Jul 100%[===================>]  19.72M  43.8MB/s    in 0.5s    \n","\n","2021-03-14 10:34:10 (43.8 MB/s) - ‘NASA_access_log_Jul95.gz’ saved [20676677/20676677]\n","\n","gzip: NASA_access_log_Jul95 already exists; do you wish to overwrite (y or n)? y\n"]}],"source":["!wget https://raw.githubusercontent.com/scalable-infrastructure/exercise-students-2021/master/data/nasa/NASA_access_log_Jul95.gz\n","!gzip -d NASA_access_log_Jul95.gz"]},{"cell_type":"markdown","metadata":{"id":"YZO5zgmrNrKK"},"source":["3.2 Implement a wordcount using Spark. How many words are in the file `example.csv`?\n","\n","3.3 Using the NASA Log file, implement a Spark version of the HTTP Response Code Analysis. How many log enteries per HTTP Response Code exist? "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L1uZRxt5TjGG"},"outputs":[],"source":[]}],"metadata":{"anaconda-cloud":{},"colab":{"collapsed_sections":["fGsih3q0NrKI","c6IFRE0kNrKJ"],"name":"04_Spark.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
