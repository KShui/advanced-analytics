{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"05_SQL.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"dkASU227TuUJ"},"source":["# 5.SQL and Dataframes\n","\n","References:\n","\n","* Spark-SQL, <https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes>\n","\n","\n","# 5.1  Example Walkthrough\n","Follow the Spark SQL and Dataframes Examples below!\n","\n","### Initialize PySpark\n","\n","First, we use the findspark package to initialize PySpark."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LbZC0TEZTuUL","executionInfo":{"status":"ok","timestamp":1615718654628,"user_tz":-60,"elapsed":38638,"user":{"displayName":"Andre Luckow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64","userId":"13897213013486729084"}},"outputId":"0fd2fdd4-1e3e-430d-f773-2a4c82ddd598"},"source":["!pip install pyspark"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n","\u001b[K     |████████████████████████████████| 212.3MB 66kB/s \n","\u001b[?25hCollecting py4j==0.10.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n","\u001b[K     |████████████████████████████████| 204kB 18.0MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=a9de2ed215394afff000148bf5ea2b6758713b069dcb86efe0289a1d60d1b95f\n","  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9 pyspark-3.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hiLvAIOTuUL","executionInfo":{"status":"ok","timestamp":1615718661273,"user_tz":-60,"elapsed":45276,"user":{"displayName":"Andre Luckow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64","userId":"13897213013486729084"}},"outputId":"6575605f-9f26-4948-ced1-837d45c43d31"},"source":["# Initialize PySpark\n","import os, sys\n","APP_NAME = \"PySpark Lecture\"\n","SPARK_MASTER=\"local[1]\"\n","import pyspark\n","import pyspark.sql\n","from pyspark.sql import Row\n","conf=pyspark.SparkConf()\n","conf=pyspark.SparkConf().setAppName(APP_NAME).set(\"spark.local.dir\", os.path.join(os.getcwd(), \"tmp\"))\n","sc = pyspark.SparkContext(master=SPARK_MASTER, conf=conf)\n","spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n","\n","print(\"PySpark initiated...\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["PySpark initiated...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"29hDbwJgTuUM"},"source":["### Hello, World!\n","\n","Loading data, mapping it and collecting the records into RAM..."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sdkuaf_VVUxy","executionInfo":{"status":"ok","timestamp":1615718661502,"user_tz":-60,"elapsed":45502,"user":{"displayName":"Andre Luckow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64","userId":"13897213013486729084"}},"outputId":"27a4ead4-a19b-47db-d10c-98e583740100"},"source":["!wget https://raw.githubusercontent.com/scalable-infrastructure/exercise-students-2021/master/data/example.csv"],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2021-03-14 10:44:21--  https://raw.githubusercontent.com/scalable-infrastructure/exercise-students-2021/master/data/example.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 189 [text/plain]\n","Saving to: ‘example.csv’\n","\n","example.csv         100%[===================>]     189  --.-KB/s    in 0s      \n","\n","2021-03-14 10:44:21 (11.7 MB/s) - ‘example.csv’ saved [189/189]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":936},"id":"DaNcpjX0TuUM","executionInfo":{"status":"error","timestamp":1615718662644,"user_tz":-60,"elapsed":46638,"user":{"displayName":"Andre Luckow","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64","userId":"13897213013486729084"}},"outputId":"99a05eae-4b22-4a78-8dbc-73f7aec104a5"},"source":["# Load the text file using the SparkContext\n","csv_lines = sc.textFile(\"../data/example.csv\")\n","\n","# Map the data to split the lines into a list\n","data = csv_lines.map(lambda line: line.split(\",\"))\n","\n","# Collect the dataset into local RAM\n","data.collect()"],"execution_count":4,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-62137bd5d311>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Collect the dataset into local RAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/data/example.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"]}]},{"cell_type":"markdown","metadata":{"id":"rKlgdJVdTuUM"},"source":["### Creating Rows\n","\n","Creating `pyspark.sql.Rows` out of your data so you can create DataFrames..."]},{"cell_type":"code","metadata":{"id":"S3ZDj3uvTuUN"},"source":["# Convert the CSV into a pyspark.sql.Row\n","def csv_to_row(line):\n","    parts = line.split(\",\")\n","    row = Row(\n","      name=parts[0],\n","      company=parts[1],\n","      title=parts[2]\n","    )\n","    return row\n","\n","# Apply the function to get rows in an RDD\n","rows = csv_lines.map(csv_to_row)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sRmw3MIBTuUN"},"source":["### Creating DataFrames from RDDs\n","\n","Using the `RDD.toDF()` method to create a dataframe, registering the `DataFrame` as a temporary table with Spark SQL, and counting the jobs per person using Spark SQL."]},{"cell_type":"code","metadata":{"id":"R0Xj9BfITuUN","outputId":"574c5ba1-51f6-4311-e47e-b847fec02906"},"source":["# Convert to a pyspark.sql.DataFrame\n","rows_df = rows.toDF()\n","\n","# Register the DataFrame for Spark SQL\n","rows_df.registerTempTable(\"executives\")\n","\n","# Generate a new DataFrame with SQL using the SparkSession\n","job_counts = spark.sql(\"\"\"\n","SELECT\n","  name,\n","  COUNT(*) AS total\n","  FROM executives\n","  GROUP BY name\n","\"\"\")\n","job_counts.show()\n","\n","# Go back to an RDD\n","job_counts.rdd.collect()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------------+-----+\n","|           name|total|\n","+---------------+-----+\n","|   Donald Trump|    1|\n","|Florian Liebert|    1|\n","|      Don Brown|    1|\n","| Russell Jurney|    2|\n","|     Steve Jobs|    1|\n","+---------------+-----+\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[Row(name='Donald Trump', total=1),\n"," Row(name='Florian Liebert', total=1),\n"," Row(name='Don Brown', total=1),\n"," Row(name='Russell Jurney', total=2),\n"," Row(name='Steve Jobs', total=1)]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"2ofD74jfTuUO"},"source":["# 5.2-5.9 NASA DataSet\n","\n","5.2 Create a Spark-SQL table with fields for IP/Host and Response Code from the NASA Log file! "]},{"cell_type":"markdown","metadata":{"id":"uELQAPjYTuUO"},"source":["5.3 Run an SQL query that outputs the number of occurrences of each HTTP response code!\n","\n","5.4 Implement the same Query using the Dataframe API!"]},{"cell_type":"markdown","metadata":{"id":"c22wAS25TuUO"},"source":["5.5 Cachen Sie den Dataframe und führen Sie dieselbe Query nochmals aus! Messen Sie die Laufzeit für das Cachen und für die Ausführungszeit der Query!"]},{"cell_type":"markdown","metadata":{"id":"DfsKn2IGTuUP"},"source":["5.6 Performance Analysis - Weak Scaling: \n","* Create RDDs with 2x, 4x, 8x and 16x of the size of the NASA log dataset! Persist the dataset in the Spark Cache! Use an appropriate number of cores (e.g. 8 or 16)!\n","* Measure and plot the response times for all datasets using a constant number of cores!\n","* Plot the results!\n","* Explain the results!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U2j_E8RSTuUP"},"source":["5.7 Performance Analysis - Weak Scaling: \n","\n","  * **Measure the runtime for the query for 1, 2, 4, 8 and 16 cores for 1x and 16x datasets!** Datasets cached in Memory!\n","  * Compute the speedup and efficiency!\n","  * Plot the responses!\n","  * Explain the results!"]}]}