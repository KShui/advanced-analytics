{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Copy of 05_SQL.ipynb","provenance":[{"file_id":"https://github.com/scalable-infrastructure/exercise-students-2021/blob/master/05_SQL/05_SQL.ipynb","timestamp":1617113592153}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"dkASU227TuUJ"},"source":["# 5.SQL and Dataframes\n","\n","References:\n","\n","* Spark-SQL, <https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes>\n","\n","\n","# 5.1  Example Walkthrough\n","Follow the Spark SQL and Dataframes Examples below!\n","\n","### Initialize PySpark\n","\n","First, we use the findspark package to initialize PySpark."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LbZC0TEZTuUL","executionInfo":{"status":"ok","timestamp":1617112910170,"user_tz":-120,"elapsed":38298,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"bcef61e9-dc5a-44a3-eff4-2943686c6b05"},"source":["!pip install pyspark"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n","\u001b[K     |████████████████████████████████| 212.3MB 70kB/s \n","\u001b[?25hCollecting py4j==0.10.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n","\u001b[K     |████████████████████████████████| 204kB 17.4MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=045f7d89d6c7ae2ed73d6c23388a9f89bbb19f074683f6eca7488589a1439995\n","  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9 pyspark-3.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PiPbzzOQcORs","executionInfo":{"status":"ok","timestamp":1617113437577,"user_tz":-120,"elapsed":673,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"190e9f8d-15e4-4ca6-9241-465753021da3","colab":{"base_uri":"https://localhost:8080/"}},"source":["!cat /proc/cpuinfo"],"execution_count":1,"outputs":[{"output_type":"stream","text":["processor\t: 0\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n","stepping\t: 3\n","microcode\t: 0x1\n","cpu MHz\t\t: 1999.999\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 2\n","core id\t\t: 0\n","cpu cores\t: 1\n","apicid\t\t: 0\n","initial apicid\t: 0\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n","bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n","bogomips\t: 3999.99\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 1\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n","stepping\t: 3\n","microcode\t: 0x1\n","cpu MHz\t\t: 1999.999\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 2\n","core id\t\t: 0\n","cpu cores\t: 1\n","apicid\t\t: 1\n","initial apicid\t: 1\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n","bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n","bogomips\t: 3999.99\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C-kL3H3scb4b","executionInfo":{"status":"ok","timestamp":1617113437578,"user_tz":-120,"elapsed":437,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":[""],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hiLvAIOTuUL","executionInfo":{"status":"ok","timestamp":1617113444953,"user_tz":-120,"elapsed":6579,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"b291a61b-2653-4758-dcc3-545ff3f96065"},"source":["# Initialize PySpark\n","import os, sys\n","APP_NAME = \"PySpark Lecture\"\n","SPARK_MASTER=\"local[2]\"\n","import pyspark\n","import pyspark.sql\n","from pyspark.sql import Row\n","conf=pyspark.SparkConf()\n","conf=pyspark.SparkConf().setAppName(APP_NAME).set(\"spark.local.dir\", os.path.join(os.getcwd(), \"tmp\"))\n","sc = pyspark.SparkContext(master=SPARK_MASTER, conf=conf)\n","spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n","\n","print(\"PySpark initiated...\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["PySpark initiated...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"29hDbwJgTuUM"},"source":["### Hello, World!\n","\n","Loading data, mapping it and collecting the records into RAM..."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sdkuaf_VVUxy","executionInfo":{"status":"ok","timestamp":1617113444953,"user_tz":-120,"elapsed":3446,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"5a714bb1-435d-435b-8a42-86ed761d2650"},"source":["!wget https://raw.githubusercontent.com/scalable-infrastructure/exercise-students-2021/master/data/example.csv"],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2021-03-30 14:10:44--  https://raw.githubusercontent.com/scalable-infrastructure/exercise-students-2021/master/data/example.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 189 [text/plain]\n","Saving to: ‘example.csv.1’\n","\n","\rexample.csv.1         0%[                    ]       0  --.-KB/s               \rexample.csv.1       100%[===================>]     189  --.-KB/s    in 0s      \n","\n","2021-03-30 14:10:44 (9.28 MB/s) - ‘example.csv.1’ saved [189/189]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DaNcpjX0TuUM","executionInfo":{"status":"ok","timestamp":1617113464836,"user_tz":-120,"elapsed":2270,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"7ec3dfde-2712-4dd6-bd2d-d278ceeabe18"},"source":["# Load the text file using the SparkContext\n","csv_lines = sc.textFile(\"example.csv\")\n","\n","# Map the data to split the lines into a list\n","data = csv_lines.map(lambda line: line.split(\",\"))\n","\n","# Collect the dataset into local RAM\n","data.collect()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['Russell Jurney', 'Relato', 'CEO'],\n"," ['Florian Liebert', 'Mesosphere', 'CEO'],\n"," ['Don Brown', 'Rocana', 'CIO'],\n"," ['Steve Jobs', 'Apple', 'CEO'],\n"," ['Donald Trump', 'The Trump Organization', 'CEO'],\n"," ['Russell Jurney', 'Data Syndrome', 'Principal Consultant']]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"rKlgdJVdTuUM"},"source":["### Creating Rows\n","\n","Creating `pyspark.sql.Rows` out of your data so you can create DataFrames..."]},{"cell_type":"code","metadata":{"id":"S3ZDj3uvTuUN","executionInfo":{"status":"ok","timestamp":1617113556308,"user_tz":-120,"elapsed":649,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# Convert the CSV into a pyspark.sql.Row\n","def csv_to_row(line):\n","    parts = line.split(\",\")\n","    row = Row(\n","      name=parts[0],\n","      company=parts[1],\n","      title=parts[2]\n","    )\n","    return row\n","\n","# Apply the function to get rows in an RDD\n","rows = csv_lines.map(csv_to_row)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sRmw3MIBTuUN"},"source":["### Creating DataFrames from RDDs\n","\n","Using the `RDD.toDF()` method to create a dataframe, registering the `DataFrame` as a temporary table with Spark SQL, and counting the jobs per person using Spark SQL."]},{"cell_type":"code","metadata":{"id":"R0Xj9BfITuUN","executionInfo":{"status":"ok","timestamp":1617113568045,"user_tz":-120,"elapsed":10320,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"1b6ec660-a930-45ea-9d2c-2f865ad707fc","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Convert to a pyspark.sql.DataFrame\n","rows_df = rows.toDF()\n","\n","# Register the DataFrame for Spark SQL\n","rows_df.registerTempTable(\"executives\")\n","\n","# Generate a new DataFrame with SQL using the SparkSession\n","job_counts = spark.sql(\"\"\"\n","SELECT\n","  name,\n","  COUNT(*) AS total\n","  FROM executives\n","  GROUP BY name\n","\"\"\")\n","job_counts.show()\n","\n","# Go back to an RDD\n","job_counts.rdd.collect()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["+---------------+-----+\n","|           name|total|\n","+---------------+-----+\n","|   Donald Trump|    1|\n","|Florian Liebert|    1|\n","|      Don Brown|    1|\n","| Russell Jurney|    2|\n","|     Steve Jobs|    1|\n","+---------------+-----+\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[Row(name='Donald Trump', total=1),\n"," Row(name='Florian Liebert', total=1),\n"," Row(name='Don Brown', total=1),\n"," Row(name='Russell Jurney', total=2),\n"," Row(name='Steve Jobs', total=1)]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"2ofD74jfTuUO"},"source":["# 5.2-5.9 NASA DataSet\n","\n","5.2 Create a Spark-SQL table with fields for IP/Host and Response Code from the NASA Log file! "]},{"cell_type":"markdown","metadata":{"id":"uELQAPjYTuUO"},"source":["5.3 Run an SQL query that outputs the number of occurrences of each HTTP response code!\n","\n","5.4 Implement the same Query using the Dataframe API!"]},{"cell_type":"markdown","metadata":{"id":"c22wAS25TuUO"},"source":["5.5 Cachen Sie den Dataframe und führen Sie dieselbe Query nochmals aus! Messen Sie die Laufzeit für das Cachen und für die Ausführungszeit der Query!"]},{"cell_type":"markdown","metadata":{"id":"DfsKn2IGTuUP"},"source":["5.6 Performance Analysis - Weak Scaling: \n","* Create RDDs with 2x, 4x, 8x and 16x of the size of the NASA log dataset! Persist the dataset in the Spark Cache! Use an appropriate number of cores (e.g. 8 or 16)!\n","* Measure and plot the response times for all datasets using a constant number of cores!\n","* Plot the results!\n","* Explain the results!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U2j_E8RSTuUP"},"source":["5.7 Performance Analysis - Weak Scaling: \n","\n","  * **Measure the runtime for the query for 1, 2, 4 cores for 1x and 16x datasets!** Datasets cached in Memory! Note that Collab environment only has two cores!\n","  * Compute the speedup and efficiency!\n","  * Plot the responses!\n","  * Explain the results!"]},{"cell_type":"code","metadata":{"id":"9_74seWVb9kW"},"source":[""],"execution_count":null,"outputs":[]}]}